<html>
<body lang=EN-US link=blue vlink=purple>
<div style="margin:50px 150px 0px 150px;">

<h1 align=center>
Coursera/John Hopkins Machine Learning Course<BR>
Feb-March, 2015
</h1>

<h2>Contents</h2>

&nbsp&nbsp<a href="#_Toc414656852">Background</a><br>
&nbsp&nbsp<a href="#_Toc414656853">Summary</a><br>
&nbsp&nbsp<a href="#_Toc414656854">Details of the Machine Learning Algorithm</a><br>
&nbsp&nbsp&nbsp&nbsp<a href="#_Toc414656855">Choice
of the method (Caret, Random Forest)</a><br>
&nbsp&nbsp&nbsp&nbsp<a href="#_Toc414656856">Data Cleaning and Feature-Reduction Steps</a><br>
&nbsp&nbsp<a href="#_Toc414656863">Cross-validation and Out-of-sample Error</a><br>
&nbsp&nbsp<a href="#_Toc414656864">Interpretations of results</a><br> 
&nbsp&nbsp<a href="#_Toc414656865">Did Removal of Correlated Columns Help and By How Much?</a><br>



<h2><a name="_Toc414656852">Background</a></h2>
<p class=MsoNormal> The goal of
this study was to determine whether the data collected by an accelerometer attached to 
the
belt, forearm, arm, and dumbbell of 
a person involved in different physical
exercises can be used to predict whether the exercise was done correctly or
not. Six human subjects were asked to
perform a physical exercise in one right way (A) and four wrong ways (B, C, D,
and E). Accelerometer data was collected
for each execution of the exercise. It
was assumed that the participants were indeed able to do the exercise in one of
these five ways when asked to do so. The
categorization problem here required the classifier to learn to identify the
way the exercise was done form the accelerometer data. The data and more details about it are
available here: <a
href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a>.

<h2><a name="_Toc414656853">Summary</a></h2>

<p> The best accuracy achieved was 99.8% with a data
size of 6,851 random training rows. This run took 2,820 seconds of
training time. After we settled with a fixed set of 35 features, we
increased the training set size from 100 (random) rows till over 6800
rows and measured the accuracy (using different and non-overlapping
test sets). Each run had 25% more rows than the previous. In each run
the test set(s) were kept completely independent from the
corresponding training set. All our runs with 737 or more training
points scored the 20 test problems with 100% accuracy indicating that
the learning process was successively stablizing. This 737-point run
took just about 180 seconds to finish learning. The following
figure displays how the accuracy and training time increased with the
training size.

<p> It was not
the goal of this experiment to beat any accuracy records. We wanted to draw
some good conclusions about one important aspect of the problem and we chose
training set size as the main aspect of our study. Generating an adequate model (details below)
was a relatively easy process. We wanted
to discover the smallest training set that will give us an accuracy of over 99%. We hit that mark at training set size 2807.

<h2><a name="_Toc414656854">Details of the Machine Learning Algorithm</a></h2>

<p> Our goal was
to take a series of reasonable decisions as we go forward but be willing to retract
those decisions if needed. Luckily we
did not have to make many backtracking decisions. Our major decisions are described below:</p>

<h3><a name="_Toc414656855"> Choice
of the method (Caret, Random Forest)</a></h3>

<p> Since it was
a classification problem involving 5 categories, a decision tree was a natural
choice. We decided to use carets random
forest method. An alternative (RPART) is inferior to random forest especially
when problem complexity is not known in advance. We decided against any NN or SVM methods
because they are less suitable for classification tasks.</p>

<h3><a name="_Toc414656856"> Data
Cleaning and Feature-Reduction Steps</a></h3>

<p> We followed
several successive steps that made sense at the moment.  Most of the times we did not have to backtrack. Here are the some of the decisions that we made:</p>

<ol>
<li><b><a name="_Toc414656857">Conversion to tbl_df form right after reading the data: Mainly done for easy
human handling.</b></li>


<li><b><a name="_Toc414656858">Removal of Statistical Summary Variables</a>:</b> We removed all
columns that matched the regular expression:<i> ^max|^min|^amplitude|^var|^avg|^stddev|^skewness|^kurtosis</i>. Statistical summaries may be useful in
certain learning situations but mostly not. We could have brought back some of these columns but it never turned out
to be necessary. Max, min, and amplitude
were three variables that we considered keeping but decided not to. (159 cols
reduced to 59)</li>

<li><b><a name="_Toc414656859">Intentional non-removal of several other variables</a>:</b> We intentionally
did not remove name and time-stamp variables from the data set for two reasons
1) Time and subject may indeed have a relation with the outcome; Some people
may be able to follow procedures more correctly than others and at certain
times 2) if there is no relationship, we want machine to discover it without
bringing human bias into the picture.</li>

<li><b><a name="_Toc414656860">Correlation-based feature removal</a>:</b>
After setting aside a test data set (discussed later), we performed a
correlation experiment. If two variables had a <i>abs(correlation)</i>
of 0.8 or more, we removed one of those randomly. This process
continued in a transitive fashion (in a chain of correlated variables
X, Y, ... Z only Z might survive). This step was common to all runs
and was done only once.(59 columns reduced to 46)</li>

<li><b><a name="_Toc414656861">PCA based feature removal</a>(tried but
not used):</b> We did not want to use PCA unless absolutely
necessary. But we used a novel approach to remove those variables that
had smallest (absolute) slopes in various PCs. So if a variable had
slopes (0.001, 0.007, -0.001 ..) in the PCs, it would be a candidate
to be removed but a variable of slopes (50, 100, and -100) will not
be. We did not pursue this path due to the risk involved. But a visual
inspection showed the promise of this approach.</li>

<li><b><a name="_Toc414656862">Experimental variable removal:</b></a>Only
using the training set data points we did a quick experiment of
generating 10 different random forests with a ten small random
training sets (300 rows each). We calculated the role played by each
variable <i>($fit$finalModel[&quot;importance&quot;])</i> in these
random forests and the in-sample accuracy of the forest. We rejected
25% of the variables that played least roles in these quick and dirty
forests. This step was also done only once. (46 columns reduced to
35)</li>
</ol>
<p> Our main
experiment used a data set with 35 columns.</p>


<h3><p><a name="_Toc414656863">Cross-validation and Out-of-sample Error</a></p></h2>

<p> In any machine learning experiment out-of-sample
(OOS) error is likely to be more than in-sample error (except when the
concept is completely learnable in which case OOS may be same as the
in-sample error).  As also indicated in the table below, in our
experiment our final model had a 0 in-sample mean squared error (MSE).
The OOS MSE was a tiny 0.002033.  In our first few models (generated
with very small training sets) both in-sample and out-of-sample errors
were larger with OOS being bigger than In-sample.
</p>
<p> 


This data
set had enough data to finish the learning task without any severe rationing of
data. So there was no need to utilize an
extensively sophisticated CV technique. We, however, took extreme care that the test sets were completely
isolated from the training sets. Here is
what was done:</p>

<p><b>Test Set 1: Completely isolated 30%
Test Subset</b>:A 30% random subset of the data was left out
and was used only for testing. Every model
generated was tested on this data set. This allowed us to compare different learning scenarios on exact same
test-data.</p>

<p><b>Test Set 2: Random test-subsets for each experiment: </b>The remaining 70% of the data was used for training and
testing. If an experiment needs N
training data points, we picked N random data points to the experiment to be
used for training. But we also opportunistically
used the remaining 70%-N points to formulate yet another test set. This latter set was also NOT used for
training. </p>

<p> Due to ample
availability of the data we never had to use methods like k-fold cross
validation or leave-one-out. </p>

<h1 align=center><img border=0 width=415 height=309
src="Writeup_files/image004.jpg" v:shapes="Picture_x0020_4"></h1>

<h2><a name="_Toc414656864"> Interpretations of results</a> </h2>

<p> We used our last (and best) model for this analysis. Here are some
pertinent details:</p>

<table>
 <tr>
  <td> &nbsp&nbsp&nbsp</td>
  <td>Features used</td>
  <td> &nbsp&nbsp&nbsp</td>
  <td>35</td>
 </tr>
 <tr>
  <td> &nbsp&nbsp&nbsp</td>
   <td>Training Set Size</td>
  <td> &nbsp&nbsp&nbsp</td>
   <td>6851</td>
 </tr>
<tr>
  <td> &nbsp&nbsp&nbsp</td>
<td>Training (clock) time</td>
  <td> &nbsp&nbsp&nbsp</td>
<td>48 Minutes (on a Dell PC running Windows 7)</td>
</tr>
<tr>
  <td> &nbsp&nbsp&nbsp</td><td>MSE (in sample)</td>
  <td> &nbsp&nbsp&nbsp</td>
<td>0</td>
<tr>
  <td> &nbsp&nbsp&nbsp</td>
<td>MSE (out of sample)</td>
  <td> &nbsp&nbsp&nbsp</td>
<td>0.002033</td>
<tr>
  <td> &nbsp&nbsp&nbsp</td>
<td>Accuracy</td>
  <td> &nbsp&nbsp&nbsp</td>
<td>99.79%</td>
</tr>
</table>



<p> <o:p>&nbsp;</o:p></p>

<h2><a name="_Toc414656865"> Did
Removal of Correlated Columns Help and By How Much?</a></h2>

<p> We wanted to
see how much did the removal of correlated columns help. The following graphs show the learning
accuracy and training times as a result of training set size for the two
cases. Removing the correlated variables
improved the accuracy only marginally but improved the learning time quite significantly.</p>

<p> <o:p>&nbsp;</o:p></p>

<p><span style='font-size:14.0pt;line-height:115%;mso-no-proof:
yes'><!--[if gte vml 1]><v:shape id="Picture_x0020_7" o:spid="_x0000_i1025"
 type="#_x0000_t75" style='width:440.25pt;height:197.25pt;visibility:visible;
 mso-wrap-style:square'>
 <v:imagedata src="Writeup_files/image005.png" o:title=""/>
</v:shape><![endif]--><![if !vml]><img border=0 width=587 height=263
src="Writeup_files/image006.jpg" v:shapes="Picture_x0020_7"><![endif]><span
style='font-size:14.0pt;line-height:115%'></p>

</div>
</body>
</html>
